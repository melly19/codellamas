from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, List, Optional

from codellamas_backend.crew import CodellamasBackend
from codellamas_backend.tools.maven_tool import MavenTool
from codellamas_backend.tools.workspace import FileLike

app = FastAPI(title="CodeLlamas Backend")

class ProjectFile(BaseModel):
    path: str
    content: str


class GenerateExerciseRequest(BaseModel):
    topic: str = "Online Shopping"
    code_smells: List[str] = Field(default_factory=lambda: ["Feature Envy"])
    seed: int = 42
    mode: Literal["single", "multi"] = "multi"
    # VS Code should send selected project files (pom.xml + relevant src/**)
    project_files: List[ProjectFile] = Field(default_factory=list)


class GenerateExerciseArtifacts(BaseModel):
    problem_md: str
    instructions_md: str = ""
    tests: Dict[str, str] = Field(default_factory=dict)      # path -> content
    solution: Dict[str, str] = Field(default_factory=dict)   # path -> content
    review_notes: str = ""


class GenerateExerciseResponse(BaseModel):
    run_id: str
    artifacts: GenerateExerciseArtifacts
    diagnostics: Dict[str, Any] = Field(default_factory=dict)


class EvaluateSubmissionRequest(BaseModel):
    # If you track run_id later, keep it here; optional for now
    run_id: Optional[str] = None

    # The opened Spring Initializr project from VS Code
    project_files: List[ProjectFile]

    # Student edits to src/main/java/** etc.
    student_files: List[ProjectFile] = []

    # Tests to run (generated by /generate or stored by extension)
    tests: Dict[str, str] = {}

    # Exercise context for LLM feedback
    problem_description: str = ""
    original_code: str = ""


class EvaluateSubmissionResponse(BaseModel):
    run_id: str
    status: Literal["PASS", "FAIL"]
    failed_tests: List[str] = Field(default_factory=list)
    errors: List[str] = Field(default_factory=list)
    raw_log_head: str = ""
    feedback: str = ""


def _make_run_id() -> str:
    # simple run id, replace with uuid if you like
    import uuid
    return str(uuid.uuid4())


def _safe_json_extract(text: str) -> dict:
    """
    Very lightweight JSON extraction:
    if your Crew output is already strict JSON, json.loads works.
    If not, this tries to find the first {...} block.
    """
    import json
    start = text.find("{")
    if start == -1:
        raise ValueError("No JSON object found in Crew output.")
    for end in range(len(text) - 1, start, -1):
        if text[end] == "}":
            candidate = text[start:end + 1]
            try:
                return json.loads(candidate)
            except Exception:
                continue
    raise ValueError("Could not parse JSON from Crew output.")


@app.post("/generate/exercise", response_model=GenerateExerciseResponse)
async def generate_exercise(body: GenerateExerciseRequest):
    run_id = _make_run_id()

    inputs = {
        "topic": body.topic,
        # keep compatibility with existing YAML expecting `code_smell`
        "code_smell": ", ".join(body.code_smells),
        "code_smells": body.code_smells,
        "seed": body.seed,
        # pack project context into a big string for the prompt (minimal for now)
        "project_context": "\n\n".join(
            [f"### FILE: {f.path}\n{f.content}" for f in body.project_files]
        ),
    }

    try:
        raw_result = CodellamasBackend().generation_crew().kickoff(inputs=inputs)
        raw_text = str(raw_result)

        # Expect your YAML task to output strict JSON with:
        # { problem_md, instructions_md, tests: {..}, solution: {..} }
        obj = _safe_json_extract(raw_text)

        artifacts = GenerateExerciseArtifacts(
            problem_md=obj.get("problem_md", raw_text),
            instructions_md=obj.get("instructions_md", ""),
            tests=obj.get("tests", {}) or {},
            solution=obj.get("solution", {}) or {},
            review_notes=obj.get("review_notes", ""),
        )

        return GenerateExerciseResponse(
            run_id=run_id,
            artifacts=artifacts,
            diagnostics={
                "mode": body.mode,
                "seed": body.seed,
                "notes": "Execution/debug loop not wired yet (next step: Maven tool).",
            },
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/evaluate/submission")
async def evaluate_submission(body: EvaluateSubmissionRequest):
    # 1) Run real mvn test in an isolated temp workspace
    tool = MavenTool(mvn_cmd="mvn", timeout_sec=120, quiet=True)

    project_files = [FileLike(path=f.path, content=f.content) for f in body.project_files]
    student_files = [FileLike(path=f.path, content=f.content) for f in body.student_files]

    test_result = tool.run_tests(
        project_files=project_files,
        override_files=student_files,
        inject_tests=body.tests,
    )

    # 2) Pass the real test output into CrewAI review_solution
    # NOTE: Your current tasks.yaml review_solution prompt does NOT mention {test_results},
    # so it wonâ€™t use it unless you add it to tasks.yaml.
    # Still safe to pass (won't break), and once you update YAML it will be used.
    inputs = {
        "problem_description": body.problem_description,
        "original_code": body.original_code,
        "student_code": "\n\n".join([f"### FILE: {f.path}\n{f.content}" for f in body.student_files]),
        "test_results": test_result.raw_log_head(6000),
        # If your YAML uses these names instead:
        "tests": body.tests,
        "student_solution": "\n\n".join([f"### FILE: {f.path}\n{f.content}" for f in body.student_files]),
    }

    try:
        review = CodellamasBackend().review_crew().kickoff(inputs=inputs)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Review crew failed: {e}")

    # 3) Return both execution result + LLM feedback
    return {
        "run_id": body.run_id,
        "status": test_result.status,
        "failed_tests": test_result.failed_tests,
        "errors": test_result.errors,
        "raw_log_head": test_result.raw_log_head(6000),
        "llm_review": str(review),
    }



class GenerateRequest(BaseModel):
    topic: str = "Online Shopping"
    code_smell: str = "Feature Envy"


class EvaluateRequest(BaseModel):
    problem_description: str
    original_code: str
    student_code: str
    test_results: str


@app.post("/generate")
async def generate_exercise_legacy(body: GenerateRequest):
    upgraded = GenerateExerciseRequest(
        topic=body.topic,
        code_smells=[body.code_smell],
        project_files=[],
        mode="multi",
    )
    return await generate_exercise(upgraded)


@app.post("/evaluate")
async def review_solution_legacy(body: EvaluateRequest):
    upgraded = EvaluateSubmissionRequest(
        problem_md=body.problem_description,
        student_files=[ProjectFile(path="STUDENT_CODE.java", content=body.student_code)],
    )
    return await evaluate_submission(upgraded)


@app.get("/")
async def root():
    return {"status": "ok"}
