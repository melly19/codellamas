from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Literal, Any

from codellamas_backend.crew import CodellamasBackend
from codellamas_backend.tools.maven_tool import MavenTool
from codellamas_backend.tools.workspace import FileLike

app = FastAPI(title="CodeLlamas Backend")

class ProjectFile(BaseModel):
    path: str
    content: str


class GenerateExerciseRequest(BaseModel):
    topic: str = "Online Shopping"
    code_smells: List[str] = Field(default_factory=lambda: ["Feature Envy"])
    seed: int = 42
    mode: Literal["single", "multi"] = "multi"
    # VS Code should send selected project files (pom.xml + relevant src/**)
    project_files: List[ProjectFile] = Field(default_factory=list)


class GenerateExerciseResponse(BaseModel):
    run_id: str
    raw_output: str
    diagnostics: Dict[str, Any] = Field(default_factory=dict)


class EvaluateSubmissionRequest(BaseModel):
    # If you track run_id later, keep it here; optional for now
    run_id: Optional[str] = None

    # The opened Spring Initializr project from VS Code
    project_files: List[ProjectFile]

    # Student edits to src/main/java/** etc.
    student_files: List[ProjectFile] = []

    # Tests to run (generated by /generate or stored by extension)
    tests: Dict[str, str] = {}

    # Exercise context for LLM feedback
    problem_description: str = ""
    original_code: str = ""


class EvaluateSubmissionResponse(BaseModel):
    run_id: str
    status: Literal["PASS", "FAIL"]
    failed_tests: List[str] = Field(default_factory=list)
    errors: List[str] = Field(default_factory=list)
    raw_log_head: str = ""
    feedback: str = ""


def _make_run_id() -> str:
    # simple run id, replace with uuid if you like
    import uuid
    return str(uuid.uuid4())

@app.get("/health")
def health():
    return {"ok": True}

@app.post("/generate/exercise", response_model=GenerateExerciseResponse)
async def generate_exercise(body: GenerateExerciseRequest):
    run_id = _make_run_id()

    inputs = {
        "topic": body.topic,
        # keep compatibility with existing YAML expecting `code_smell`
        "code_smells": body.code_smells,
        "seed": body.seed,
        # pack project context into a big string for the prompt (minimal for now)
        "project_context": "\n\n".join(
            [f"### FILE: {f.path}\n{f.content}" for f in body.project_files]
        ),
    }

    try:
        raw_result = CodellamasBackend().generation_crew().kickoff(inputs=inputs)
        raw_text = str(raw_result)

        return GenerateExerciseResponse(
            run_id=run_id,
            raw_output=raw_text,
            diagnostics={
                "mode": body.mode,
                "seed": body.seed,
                "notes": "Returning raw LLM output (no JSON parsing / repair).",
            },
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/evaluate/submission")
async def evaluate_submission(body: EvaluateSubmissionRequest):
    # 1) Run real mvn test in an isolated temp workspace
    # tool = MavenTool(mvn_cmd="mvn", timeout_sec=180, quiet=True)
    tool = MavenTool(mvn_cmd = "mvn.cmd", timeout_sec=180, quiet=True)

    project_files = [FileLike(path=f.path, content=f.content) for f in body.project_files]
    student_files = [FileLike(path=f.path, content=f.content) for f in body.student_files]

    test_result = tool.run_tests(
        project_files=project_files,
        override_files=student_files,
        inject_tests=body.tests,
    )

    # 2) Pass the real test output into CrewAI review_solution
    # NOTE: Your current tasks.yaml review_solution prompt does NOT mention {test_results},
    # so it wonâ€™t use it unless you add it to tasks.yaml.
    # Still safe to pass (won't break), and once you update YAML it will be used.
    inputs = {
        "problem_description": body.problem_description,
        "original_code": body.original_code,
        "student_code": "\n\n".join([f"### FILE: {f.path}\n{f.content}" for f in body.student_files]),
        "test_results": test_result.raw_log_head(6000),
        "seed": 42,
        # If your YAML uses these names instead:
        "tests": body.tests,
        "student_solution": "\n\n".join([f"### FILE: {f.path}\n{f.content}" for f in body.student_files]),
        
        # TODO: Add code smell and reference solution to be ingested as inputs
    }

    review_text = ""

    try:
        review_raw = CodellamasBackend().review_crew().kickoff(inputs=inputs)
        review_text = str(review_raw)
    except Exception as e:
        review_text = f"LLM review failed: {e}"


    # 3) Return both execution result + LLM feedback
    return {
        "run_id": body.run_id,
        "status": test_result.status,
        "failed_tests": test_result.failed_tests,
        "errors": test_result.errors,
        "raw_log_head": test_result.raw_log_head(6000),
        "feedback": review_text,
    }



class GenerateRequest(BaseModel):
    topic: str
    code_smells: List[str]


class EvaluateRequest(BaseModel):
    problem_description: str
    original_code: str
    student_code: str
    test_results: str
    reference_solution: str
    code_smell: str


@app.post("/generate")
async def generate_exercise_legacy(body: GenerateRequest):
    upgraded = GenerateExerciseRequest(
        topic=body.topic,
        code_smells=body.code_smells,
        project_files=[],
        mode="multi",
    )
    return await generate_exercise(upgraded)

@app.post("/evaluate")
async def review_solution_legacy(body: EvaluateRequest):
    inputs = {
        "problem_description": body.problem_description,
        "original_code": body.original_code,
        "student_code": body.student_code,
        "test_results": body.test_results,
        "reference_solution": body.reference_solution,
        "code_smell": body.code_smell,
        "seed": 42,
    }
    try:
        raw = CodellamasBackend().review_crew().kickoff(inputs=inputs)
        return {"feedback": str(raw)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Review crew failed: {e}")


@app.get("/")
async def root():
    return {"status": "ok"}
