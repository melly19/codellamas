# --- GENERATION CREW TASKS ---
define_problem:
  description: >
    INPUTS:
    - {topic}: The business domain for the Spring Boot project (e.g., "e-commerce")
    - {code_smells}: A list of specific code smells that will be tested for this exercise (e.g., "Long Method")
    - {existing_codebase}: OPTIONAL. A structured string containing the file paths and content of an existing Spring Boot project. Use "NONE" if a new project should be generated.

    Generate a realistic software problem description in the given business domain. 
    If an existing codebase is provided, adapt the problem to its structure.
    Clearly state the refactoring task and constraints. Do not write any code or tests.
  expected_output: >
    A problem description with refactoring goals and constraints.
  agent: problem_architect

define_tests:
  description: >
    Using the **Problem Description** provided in the previous task, design a JUnit 5 test suite that defines the expected external behaviour.
    
    Invent all required class names, method signatures, and edge cases based on the requirements.
    Tests must be robust enough to serve as the "Contract" that both the smelly code and clean code must satisfy.
  expected_output: >
    A complete set of JUnit 5 test files with package declarations.
  agent: test_engineer

implement_smelly_code:
  description: >
    Review the **Problem Description** and the **JUnit 5 Test Suite** from the previous tasks.
    
    Implement Java code that:
    1. PASSES all the provided tests.
    2. Uses the EXACT class names and method signatures defined in the tests.
    3. Intentionally exhibits the specific **Code Smells** requested in the problem description.
    4. Identify which Java files students should edit to fix the code smells (typically 1-2 files).
    
    OUTPUT FORMAT (when this is the last task in a crew):
    Return a JSON object with: problem_description (from define_problem), project_files (your implementation), 
    test_files (from define_tests), solution_explanation_md (brief note that this is initial version), 
    paths_to_ex (list of file paths students must edit), and answers_list (empty for now).
  expected_output: >
    Java source files containing the functional but smelly implementation, paths_to_ex list, 
    and complete exercise structure when used as final task in initial crew.
  agent: smelly_developer

run_tests_on_smelly_code:
  description: >
    Using the **JUnit Test Suite** and the **Smelly Code** implementation from the previous tasks:
    
    1. Compile the code.
    2. Execute the tests.
    3. Analyze the output.
    
    Report any compilation errors or test failures with full diagnostics (stack traces/error messages).
  expected_output: >
    A test report including errors, stack traces, or success confirmation.
  agent: test_runner

patch_smelly_code:
  description: >
    Review the **Test Report** from the previous task. 
    
    If there are failures, fix the **Smelly Code** to make the tests pass.
    CRITICAL: Fix only what is required for correctness. Do NOT refactor or clean up the code smells; they must remain for the student to fix.
  expected_output: >
    Updated smelly Java source files that pass all tests.
  agent: debug_specialist

generate_answers_list:
  description: >
    Using the **Problem Description** and the **JUnit 5 Test Suite** (the authoritative source of truth):
    
    Produce a CLEAN, refactored "Reference Solution" implementation.
    1. It must remove the code smells present in the smelly version.
    2. It must use the EXACT same public API (class names/methods) so it passes the same tests.
    3. Use Design Patterns and SOLID principles.
    4. Provide the solution as an answers_list containing ONLY the files that students need to submit (just file name, no parent directory).
  expected_output: >
    Refactored Java source files representing the reference solution, provided as answers_list (file name only, without directory paths).
  agent: answers_list_developer

run_tests_on_answers_list:
  description: >
    Using the **JUnit Test Suite** and the **Reference Solution** code from the previous task:
    
    Compile and execute the tests. Report any compilation errors or failing tests with full diagnostics.
  expected_output: >
    A test report indicating success or detailing compilation/test failures.
  agent: test_runner

patch_answers_list:
  description: >
    Review the **Test Report** for the **Reference Solution**.
    
    If there are failures, fix the code to ensure all tests pass.
    Maintain clean code principles. Refactoring for clarity is allowed if required to fix correctness.
  expected_output: >
    Updated reference solution that passes all tests.
  agent: debug_specialist

audit_exercise:
  description: >
    Review the entire exercise package created by the crew:
    1. The **Smelly Code** (Student Version).
    2. The **Reference Solution** (Teacher Version).
    3. The **Test Suite**.
    
    Verify functional equivalence (do they both do the same thing?) and pedagogical contrast (is the smelly code actually messy compared to the clean code?).
    Ensure the specific **Code Smells** requested by the Architect are present.
    
    OUTPUT FORMAT:
    1. problem_description: Short description. Java file student need to edit. Short definition of each code smell. Format should be in markdown.
    2. project_files: (with code smell) - Full set of files for a new project without comments (including pom.xml).
    3. test_files: JUnit 5 test cases.
    4. solution_explanation_md: How to solve the exercise in markdown.
    5. paths_to_ex: List of exercise file paths that student has to edit.
    6. answers_list: List of answer files (solved exercises in Java format, just file name no parent directory).
  expected_output: >
    A quality assurance report validating the educational value of the exercise, structured as a JSON object with problem_description, project_files, test_files, solution_explanation_md, paths_to_ex, and answers_list.
  agent: quality_assurance

# --- REVIEW CREW TASKS ---
check_functional_correctness:
  description: >
    INPUTS:
    - {student_code}: The student's submitted code
    - {test_suite}: The original test suite
    
    Compile and run the tests against the student code. Identify any functional regressions.
  expected_output: >
    A functional correctness assessment.
  agent: test_runner

evaluate_code_quality:
  description: >
    INPUTS:
    - {problem_description}: The original exercise description
    - {original_code}: The initial smelly code
    - {student_code}: The student's submission
    - {answers_list}: The model solution
    - {code_smells}: A list of code smells that were tested in the question
    
    Evaluate whether the targeted smells were mitigated and assess overall code quality compared to the reference solution.
  expected_output: >
    A structured code quality review.
  agent: quality_assurance

generate_review_feedback:
  description: >
    Using the **Functional Assessment** and **Code Quality Review** from the previous tasks:
    
    Produce concise feedback for the student, an overall verdict (Pass/Fail), and a numerical rating.
  expected_output: >
    Final review feedback, verdict, and rating.
  agent: quality_assurance